{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 align='center'><font color='purple'>Improving on Benchmark</font></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ideas:\n",
    "\n",
    "----\n",
    "\n",
    "So .21 is a decent score but I need a .229 to be in the bronze range and to really be safely in that range I need closer to a .25. So, I need some ideas on how to improve the model.\n",
    "\n",
    "1) Explore optimizing the tradeoff between # of datapoints to train on and # of predictors\n",
    "\n",
    "2) Integrate PCA into the model, increase number of predictors.\n",
    "\n",
    "3) Explore alternate techniques for filling NaN values.\n",
    "\n",
    "4) Do some more detailed data exploration now that I have fewer predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Exploration Cont.\n",
    "\n",
    "----\n",
    "\n",
    "There are still too many numerical predictors for me to go through each of them, think about them analyze them etc. Luckily there are only a few categorical variables in my model now and doing some exploration on those is very doable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#import packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "\n",
    "#change working directory to current directory\n",
    "curDir = os.getcwd()\n",
    "os.chdir(curDir)\n",
    "\n",
    "#which columns I want to import\n",
    "categoricalPredictors = ['L0_S9_F204', 'L0_S9_F179', 'L0_S9_F159', 'L0_S9_F199', 'L1_S24_F710',\n",
    "       'L2_S26_F3099', 'L1_S24_F705', 'L1_S24_F675', 'L3_S32_F3851',\n",
    "       'L3_S32_F3854','Id']\n",
    "\n",
    "#import subset of our data\n",
    "nrows = 30000\n",
    "n_rows_in_file = sum(1 for row in open('train_categorical.csv'))-1\n",
    "skips = random.sample(range(1,n_rows_in_file),n_rows_in_file-nrows)\n",
    "train_categorical = pd.read_csv('train_categorical.csv',usecols = categoricalPredictors,skiprows = skips,low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Id                  0\n",
       "L0_S9_F159      29990\n",
       "L0_S9_F179      29990\n",
       "L0_S9_F199      29990\n",
       "L0_S9_F204      29990\n",
       "L1_S24_F675     29445\n",
       "L1_S24_F705     29445\n",
       "L1_S24_F710     29445\n",
       "L2_S26_F3099    24165\n",
       "L3_S32_F3851    29443\n",
       "L3_S32_F3854    29443\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_categorical.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay so for the vast majority of these columns are empty despite them being strong predictors.\n",
    "\n",
    "These numbers led me to look back at my benchmarks and I realized I do not have a large enough testing set to have reliable results on this one. I need to reimport my benchmark model and properly test it.\n",
    "\n",
    "Since these categorical variables are empty for almost all the predictions i'll just use the numeric predictors since I need the extra memory to get reliable evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "numericalPredictors = ['L0_S12_F344', 'L0_S21_F522', 'L0_S12_F330', 'L0_S23_F663',\n",
    "       'L1_S25_F2164', 'L0_S21_F517', 'L1_S24_F1743', 'L0_S12_F342',\n",
    "       'L1_S24_F1798', 'L0_S22_F546', 'L3_S30_F3779', 'L0_S15_F406',\n",
    "       'L1_S24_F1846', 'L3_S36_F3938', 'L0_S12_F334', 'L0_S21_F507',\n",
    "       'L3_S36_F3930', 'L1_S24_F1844', 'L0_S22_F571', 'L1_S25_F2484',\n",
    "       'L0_S15_F415', 'L0_S23_F627', 'L3_S35_F3903', 'L0_S23_F619',\n",
    "       'L2_S27_F3144', 'L0_S3_F76', 'L0_S23_F655', 'L1_S25_F2021',\n",
    "       'L2_S27_F3206', 'L0_S15_F397', 'L1_S24_F1831', 'L0_S17_F433',\n",
    "       'L0_S17_F431', 'L1_S24_F1773', 'L0_S18_F449', 'L0_S3_F68',\n",
    "       'L1_S24_F1647', 'L0_S7_F136', 'L1_S24_F872', 'L0_S21_F532',\n",
    "       'L0_S9_F175', 'L0_S12_F338', 'L0_S21_F502', 'L1_S24_F1778',\n",
    "       'L0_S12_F336', 'L0_S10_F234', 'L0_S2_F56', 'L1_S25_F2051',\n",
    "       'L0_S10_F239', 'L2_S26_F3040', 'L0_S22_F551', 'L0_S14_F390',\n",
    "       'L0_S21_F527', 'L0_S14_F374', 'L1_S24_F1516', 'L0_S12_F332',\n",
    "       'L1_S24_F1824', 'L0_S21_F537', 'L0_S2_F40', 'L1_S24_F1763',\n",
    "       'L1_S24_F1812', 'L0_S23_F667', 'L1_S25_F2960', 'L1_S24_F1518',\n",
    "       'L0_S14_F386', 'L1_S24_F683', 'L3_S36_F3926', 'L2_S27_F3166',\n",
    "       'L1_S24_F1520', 'L0_S3_F92', 'L1_S24_F1808', 'L0_S15_F403',\n",
    "       'L1_S25_F2828', 'L0_S12_F340', 'L1_S24_F1667', 'L0_S11_F298',\n",
    "       'L3_S29_F3407', 'L0_S11_F310', 'L0_S19_F459', 'L0_S15_F418',\n",
    "       'L0_S13_F356', 'L0_S10_F244', 'L1_S24_F1758', 'L0_S10_F249',\n",
    "       'L0_S11_F314', 'L0_S14_F362', 'L0_S19_F455', 'L0_S22_F561',\n",
    "       'L0_S9_F190', 'L0_S0_F14', 'L1_S24_F1512', 'L1_S24_F1829',\n",
    "       'L0_S10_F274', 'L1_S24_F1265', 'L0_S9_F165', 'L1_S24_F1569',\n",
    "       'L0_S10_F224', 'L1_S24_F1000', 'L0_S9_F200', 'L0_S11_F306',\n",
    "       'L1_S24_F1010', 'L3_S36_F3934', 'L0_S11_F318', 'L0_S12_F350',\n",
    "       'L0_S9_F170', 'L1_S24_F1637', 'L0_S10_F229', 'L1_S24_F1728',\n",
    "       'L3_S30_F3709', 'L1_S24_F1498', 'L1_S24_F1733', 'L1_S24_F1848',\n",
    "       'L1_S24_F1573', 'L0_S9_F180', 'L3_S35_F3894', 'L0_S10_F259',\n",
    "       'L3_S30_F3684', 'L0_S9_F185', 'L3_S44_F4121', 'L0_S12_F352',\n",
    "       'L3_S34_F3882', 'L0_S5_F114', 'L0_S0_F6', 'L2_S26_F3036',\n",
    "       'L0_S0_F12', 'L3_S33_F3873', 'L0_S2_F48', 'L0_S6_F132',\n",
    "       'L0_S11_F282', 'L1_S24_F1685', 'L0_S11_F326', 'L0_S3_F84',\n",
    "       'L2_S26_F3073', 'L2_S26_F3106', 'L3_S30_F3624', 'L0_S0_F10',\n",
    "       'L0_S12_F348', 'L3_S36_F3918', 'L2_S26_F3113', 'L1_S24_F1690',\n",
    "       'L0_S4_F104', 'L1_S24_F1850', 'L3_S36_F3922', 'L1_S25_F2767',\n",
    "       'L0_S9_F195', 'L1_S24_F1695', 'L3_S34_F3880', 'L3_S30_F3664',\n",
    "       'L1_S25_F2007', 'L0_S3_F100', 'L0_S11_F286', 'L0_S0_F4',\n",
    "       'L2_S26_F3047', 'L3_S33_F3869', 'L3_S29_F3412', 'L0_S0_F2',\n",
    "       'L3_S29_F3404', 'L2_S26_F3121', 'L0_S6_F122', 'L0_S9_F210',\n",
    "       'L2_S26_F3062', 'L2_S26_F3117', 'L2_S26_F3051', 'L3_S29_F3461',\n",
    "       'L0_S9_F160', 'L0_S14_F370', 'L3_S33_F3867', 'L1_S24_F1571',\n",
    "       'L0_S22_F556', 'L3_S34_F3878', 'L0_S11_F294', 'L3_S30_F3584',\n",
    "       'L0_S10_F264', 'L0_S0_F0', 'L0_S10_F219', 'L3_S29_F3476',\n",
    "       'L0_S16_F421','Response','Id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "n = sum(1 for line in open('train_numeric.csv')) - 1 #number of records in file (excludes header)\n",
    "s = 500000 #desired sample size\n",
    "skip = sorted(random.sample(range(1,n+1),n-s)) #the 0-indexed header will not be included in the skip list\n",
    "train_numeric = pd.read_csv('train_numeric.csv',usecols = numericalPredictors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#fill in nan values\n",
    "train_numeric = train_numeric.fillna(train_numeric.mean())\n",
    "\n",
    "#format data for sklearn\n",
    "X = train_numeric.drop('Response',1)\n",
    "Y = train_numeric['Response']\n",
    "#create training and testSets\n",
    "numTestRows = 400000\n",
    " #shuffle rows\n",
    "shuffle = np.random.permutation(len(X))\n",
    "X = X.iloc[shuffle]\n",
    "Y = Y.iloc[shuffle]\n",
    "Xt = X[len(X)-numTestRows:]\n",
    "Yt = Y[len(Y)-numTestRows:]\n",
    "X = X[:len(X)-numTestRows]\n",
    "Y = Y[:len(Y)-numTestRows]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#train random forest\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "excludedForest = RandomForestClassifier(n_estimators=100,max_depth = 8,n_jobs=-1)\n",
    "excludedForest.fit(X,Y)\n",
    "\n",
    "#Evaluate forest\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "predictions = excludedForest.predict(Xt)\n",
    "matthew= matthews_corrcoef(Yt,predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.049423437741575406"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matthew"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Okay with 400k test data we get a much more stable MCC. I wasn't able to make big improvements to my forest via tuning. I feel like i'm losing a lot of data by only training on less than a tenth of the data I have. If I had like twenty predictors I could train on all one million datapoints. Lets see if I really cant make PCA work. \n",
    "\n",
    "From my data exploration notebook I know I was able to capture roughly 80% of the total variance in the dataset with 40 predictors so lets start with 40 components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "train_numeric = pd.read_csv('train_numeric.csv', usecols = numericalPredictors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#standardize my data first\n",
    "train_numeric = train_numeric.fillna(train_numeric.mean())\n",
    "Y = train_numeric['Response']\n",
    "train_numeric = (train_numeric - train_numeric.mean())\n",
    "X = train_numeric.drop('Response',1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca_analysis = PCA(n_components = 20)\n",
    "pca_analysis.fit(X)\n",
    "X = pca_analysis.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#create training and testSets\n",
    "X = pd.DataFrame(X)\n",
    "numTestRows = 400000\n",
    " #shuffle rows\n",
    "shuffle = np.random.permutation(len(X))\n",
    "X = X.iloc[shuffle]\n",
    "Y = Y.iloc[shuffle]\n",
    "Xt = X[len(X)-numTestRows:]\n",
    "Yt = Y[len(Y)-numTestRows:]\n",
    "X = X[:len(X)-numTestRows]\n",
    "Y = Y[:len(Y)-numTestRows]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#train random forest\n",
    "matthew = []\n",
    "for i in range(1,10):\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    excludedForest = RandomForestClassifier(n_estimators=10,max_depth = i,n_jobs=-1)\n",
    "    excludedForest.fit(X,Y)\n",
    "\n",
    "    #Evaluate forest\n",
    "    from sklearn.metrics import matthews_corrcoef\n",
    "    predictions = excludedForest.predict(Xt)\n",
    "    matthew.append(matthews_corrcoef(Yt,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.00012074064747062272,\n",
       " 0.0,\n",
       " -0.00012074064747061976,\n",
       " -0.00012074064747062272,\n",
       " 0.023839104985199817]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matthew"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These scores are pretty terrible. PCA doesn't seem to be the way to go, maybe there is not a lot of multicolinearity between these two variables. Lets try xgboost since it maybe tolerate the missing values much better than my model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "train_numeric = pd.read_csv('train_numeric.csv',usecols = numericalPredictors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "X = train_numeric.drop('Response',1)\n",
    "Y = train_numeric['Response']\n",
    "#create training and testSets\n",
    "numTestRows = 400000\n",
    " #shuffle rows\n",
    "shuffle = np.random.permutation(len(X))\n",
    "X = X.iloc[shuffle]\n",
    "Y = Y.iloc[shuffle]\n",
    "Xt = X[len(X)-numTestRows:]\n",
    "Yt = Y[len(Y)-numTestRows:]\n",
    "X = X[:len(X)-numTestRows]\n",
    "Y = Y[:len(Y)-numTestRows]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#train xgboost\n",
    "matthew = []\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "excludedForest = XGBClassifier(max_depth=6, base_score=0.005, n_estimators = 40)\n",
    "excludedForest.fit(X.values,Y.values)\n",
    "\n",
    "#Evaluate forest\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "predictions = (excludedForest.predict_proba(Xt.values)[:,1] > .18 ).astype(np.int8)\n",
    "matthew.append(matthews_corrcoef(Yt,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.19094652866593009]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matthew"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Okay not bad, xgboost is doing waaaay better, i'm definitely honing in on the level of accuracy that can be achieved via public kernals, too bad i dont have the parallel implementation of xgboost installed."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
