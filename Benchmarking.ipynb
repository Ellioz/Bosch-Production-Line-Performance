{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 align ='center'><font color='purple'>Creating Benchmark Models </font></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Motivation:\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that I have a few basic ideas about what variables to potentially include/exclude from the dataset I want to compare the quality of a data exclusion technique to that of PCA. I'll run both techniques through a random forest classifier and compare the 'matthews correlation coefficient' of both benchmarks. Maybe i'll take a look at a combination of the two as well.\n",
    "\n",
    "Note: If I end up using excluding predictors I need to determine, somehow, whether I want fewer predictors with more data or more data with fewer predictors.Hopefully i'll be able to import all of the 'good' predictors and the decision will be easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#import modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import csv\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#change working directory to current directory\n",
    "curdir = os.getcwd()\n",
    "os.chdir(curdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Exclusion Model:\n",
    "\n",
    "----\n",
    "\n",
    "To help make the decision about whether more data fewer predictors is better than more predictors w/ less data i'll try two exclusion techniques. The first, the one im betting on, is to exclude predictors in favor of more data but I will also try retaining all predictors and sacrificing the number of datapoints I import."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Predictor names to keep (from data exploration notebook)\n",
    "numericalPredictors = ['L0_S12_F344', 'L0_S21_F522', 'L0_S12_F330', 'L0_S23_F663',\n",
    "       'L1_S25_F2164', 'L0_S21_F517', 'L1_S24_F1743', 'L0_S12_F342',\n",
    "       'L1_S24_F1798', 'L0_S22_F546', 'L3_S30_F3779', 'L0_S15_F406',\n",
    "       'L1_S24_F1846', 'L3_S36_F3938', 'L0_S12_F334', 'L0_S21_F507',\n",
    "       'L3_S36_F3930', 'L1_S24_F1844', 'L0_S22_F571', 'L1_S25_F2484',\n",
    "       'L0_S15_F415', 'L0_S23_F627', 'L3_S35_F3903', 'L0_S23_F619',\n",
    "       'L2_S27_F3144', 'L0_S3_F76', 'L0_S23_F655', 'L1_S25_F2021',\n",
    "       'L2_S27_F3206', 'L0_S15_F397', 'L1_S24_F1831', 'L0_S17_F433',\n",
    "       'L0_S17_F431', 'L1_S24_F1773', 'L0_S18_F449', 'L0_S3_F68',\n",
    "       'L1_S24_F1647', 'L0_S7_F136', 'L1_S24_F872', 'L0_S21_F532',\n",
    "       'L0_S9_F175', 'L0_S12_F338', 'L0_S21_F502', 'L1_S24_F1778',\n",
    "       'L0_S12_F336', 'L0_S10_F234', 'L0_S2_F56', 'L1_S25_F2051',\n",
    "       'L0_S10_F239', 'L2_S26_F3040', 'L0_S22_F551', 'L0_S14_F390',\n",
    "       'L0_S21_F527', 'L0_S14_F374', 'L1_S24_F1516', 'L0_S12_F332',\n",
    "       'L1_S24_F1824', 'L0_S21_F537', 'L0_S2_F40', 'L1_S24_F1763',\n",
    "       'L1_S24_F1812', 'L0_S23_F667', 'L1_S25_F2960', 'L1_S24_F1518',\n",
    "       'L0_S14_F386', 'L1_S24_F683', 'L3_S36_F3926', 'L2_S27_F3166',\n",
    "       'L1_S24_F1520', 'L0_S3_F92', 'L1_S24_F1808', 'L0_S15_F403',\n",
    "       'L1_S25_F2828', 'L0_S12_F340', 'L1_S24_F1667', 'L0_S11_F298',\n",
    "       'L3_S29_F3407', 'L0_S11_F310', 'L0_S19_F459', 'L0_S15_F418',\n",
    "       'L0_S13_F356', 'L0_S10_F244', 'L1_S24_F1758', 'L0_S10_F249',\n",
    "       'L0_S11_F314', 'L0_S14_F362', 'L0_S19_F455', 'L0_S22_F561',\n",
    "       'L0_S9_F190', 'L0_S0_F14', 'L1_S24_F1512', 'L1_S24_F1829',\n",
    "       'L0_S10_F274', 'L1_S24_F1265', 'L0_S9_F165', 'L1_S24_F1569',\n",
    "       'L0_S10_F224', 'L1_S24_F1000', 'L0_S9_F200', 'L0_S11_F306',\n",
    "       'L1_S24_F1010', 'L3_S36_F3934', 'L0_S11_F318', 'L0_S12_F350',\n",
    "       'L0_S9_F170', 'L1_S24_F1637', 'L0_S10_F229', 'L1_S24_F1728',\n",
    "       'L3_S30_F3709', 'L1_S24_F1498', 'L1_S24_F1733', 'L1_S24_F1848',\n",
    "       'L1_S24_F1573', 'L0_S9_F180', 'L3_S35_F3894', 'L0_S10_F259',\n",
    "       'L3_S30_F3684', 'L0_S9_F185', 'L3_S44_F4121', 'L0_S12_F352',\n",
    "       'L3_S34_F3882', 'L0_S5_F114', 'L0_S0_F6', 'L2_S26_F3036',\n",
    "       'L0_S0_F12', 'L3_S33_F3873', 'L0_S2_F48', 'L0_S6_F132',\n",
    "       'L0_S11_F282', 'L1_S24_F1685', 'L0_S11_F326', 'L0_S3_F84',\n",
    "       'L2_S26_F3073', 'L2_S26_F3106', 'L3_S30_F3624', 'L0_S0_F10',\n",
    "       'L0_S12_F348', 'L3_S36_F3918', 'L2_S26_F3113', 'L1_S24_F1690',\n",
    "       'L0_S4_F104', 'L1_S24_F1850', 'L3_S36_F3922', 'L1_S25_F2767',\n",
    "       'L0_S9_F195', 'L1_S24_F1695', 'L3_S34_F3880', 'L3_S30_F3664',\n",
    "       'L1_S25_F2007', 'L0_S3_F100', 'L0_S11_F286', 'L0_S0_F4',\n",
    "       'L2_S26_F3047', 'L3_S33_F3869', 'L3_S29_F3412', 'L0_S0_F2',\n",
    "       'L3_S29_F3404', 'L2_S26_F3121', 'L0_S6_F122', 'L0_S9_F210',\n",
    "       'L2_S26_F3062', 'L2_S26_F3117', 'L2_S26_F3051', 'L3_S29_F3461',\n",
    "       'L0_S9_F160', 'L0_S14_F370', 'L3_S33_F3867', 'L1_S24_F1571',\n",
    "       'L0_S22_F556', 'L3_S34_F3878', 'L0_S11_F294', 'L3_S30_F3584',\n",
    "       'L0_S10_F264', 'L0_S0_F0', 'L0_S10_F219', 'L3_S29_F3476',\n",
    "       'L0_S16_F421','Response','Id']\n",
    "\n",
    "categoricalPredictors = ['L0_S9_F204', 'L0_S9_F179', 'L0_S9_F159', 'L0_S9_F199', 'L1_S24_F710',\n",
    "       'L2_S26_F3099', 'L1_S24_F705', 'L1_S24_F675', 'L3_S32_F3851',\n",
    "       'L3_S32_F3854','Id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#import data\n",
    "n_rows = 50000\n",
    "n_rows_in_file = sum(1 for row in open('train_numeric.csv'))-1\n",
    "skips = random.sample(range(1,n_rows_in_file),(n_rows_in_file-n_rows))\n",
    "train_numeric = pd.read_csv('train_numeric.csv',usecols = numericalPredictors,skiprows = skips)\n",
    "train_categorical = pd.read_csv('train_categorical.csv',usecols = categoricalPredictors,skiprows = skips,low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#convert categorical variables to numeric\n",
    "for predictor in list(train_categorical.columns.values):\n",
    "    uniqueVals = train_categorical[predictor].unique()\n",
    "    train_categorical[predictor] = train_categorical[predictor].replace(uniqueVals,range(0,len(uniqueVals)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#join dataframes\n",
    "train_numeric['Id'] = train_categorical['Id']\n",
    "train_numeric = train_numeric.merge(train_categorical,how='inner',on='Id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#fill in nan values\n",
    "train_numeric = train_numeric.fillna(train_numeric.mean())\n",
    "\n",
    "#format data for sklearn\n",
    "X = train_numeric.drop('Response',1)\n",
    "Y = train_numeric['Response']\n",
    "#create training and testSets\n",
    "numTestRows = 10000\n",
    " #shuffle rows\n",
    "shuffle = np.random.permutation(len(X))\n",
    "X = X.iloc[shuffle]\n",
    "Y = Y.iloc[shuffle]\n",
    "Xt = X[len(X)-numTestRows:]\n",
    "Yt = Y[len(Y)-numTestRows:]\n",
    "X = X[:len(X)-numTestRows]\n",
    "Y = Y[:len(Y)-numTestRows]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train random forest\n",
    "excludedForest = RandomForestClassifier(n_estimators=100)\n",
    "excludedForest.fit(X,Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Evaluate forest\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "predictions = excludedForest.predict(Xt)\n",
    "matthew = matthews_corrcoef(Yt,predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matthew"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".21... not bad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA Model:\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#import data\n",
    "n_rows = 10000\n",
    "n_rows_in_file = sum(1 for row in open('train_numeric.csv'))-1\n",
    "skips = random.sample(range(1,n_rows_in_file),(n_rows_in_file-n_rows))\n",
    "train_numeric = pd.read_csv('train_numeric.csv',skiprows = skips)\n",
    "train_categorical = pd.read_csv('train_categorical.csv',skiprows = skips,low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#convert categorical variables to numeric\n",
    "for predictor in list(train_categorical.columns.values):\n",
    "    uniqueVals = train_categorical[predictor].unique()\n",
    "    train_categorical[predictor] = train_categorical[predictor].replace(uniqueVals,range(0,len(uniqueVals)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#join dataframes\n",
    "train_numeric['Id'] = train_categorical['Id']\n",
    "train_numeric = train_numeric.merge(train_categorical,how='inner',on='Id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#fill in nan values\n",
    "train_numeric = train_numeric.fillna(train_numeric.mean())\n",
    "train_numeric = train_numeric.drop('Id',1)\n",
    "#format data for sklearn\n",
    "X = train_numeric.drop('Response',1)\n",
    "Y = train_numeric['Response']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PCA(copy=True, n_components=50, whiten=False)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "analysis = PCA(n_components=50)\n",
    "analysis.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "newX = analysis.transform(X)\n",
    "newX = pd.DataFrame(newX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#create training and testSets\n",
    "numTestRows = 2000\n",
    " #shuffle rows\n",
    "shuffle = np.random.permutation(len(newX))\n",
    "newX = newX.iloc[shuffle]\n",
    "Y = Y[shuffle]\n",
    "\n",
    "newXT = newX[len(newX)-numTestRows:]\n",
    "Yt = Y[len(Y)-numTestRows:]\n",
    "newX = newX[:len(newX)-numTestRows]\n",
    "Y = Y[:len(Y)-numTestRows]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train random forest\n",
    "excludedForest = RandomForestClassifier(n_estimators=100)\n",
    "excludedForest.fit(newX,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import matthews_corrcoef\n",
    "predictions = excludedForest.predict(newXT)\n",
    "matthew = matthews_corrcoef(Yt,predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matthew"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Hm.... this seems too bad to be accurate. Maybe i'm doing something wrong. I'll have to come back and reexamine this idea later"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
